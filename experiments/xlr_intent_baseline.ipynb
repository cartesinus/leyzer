{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOyLYv1vmNq3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers==4.28.1 datasets seqeval optuna #tensorboard matplotlib pandas sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "L4HCN1GNmeqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "mrUR0crrmgYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "AnnJCdlgmheF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"xlm-roberta-base\"\n",
        "dataset_id = \"cartesinus/leyzer-fedcsis\"\n",
        "dataset_configs=[\"pl-PL\"]\n",
        "\n",
        "\n",
        "repository_id = \"fedcsis-intent_baseline-xlm_r-pl\""
      ],
      "metadata": {
        "id": "fxmoofV7mjNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "\n",
        "# the columns we want to keep in the dataset\n",
        "keep_columns = [\"utterance\", \"intent\"]\n",
        "\n",
        "# process individuell datasets\n",
        "proc_lan_dataset_list=[]\n",
        "for lang in dataset_configs:\n",
        "    # load dataset for language\n",
        "    lang_ds = load_dataset(dataset_id, lang)\n",
        "    # only keep the 'utt' & 'scenario column\n",
        "    lang_ds = lang_ds.remove_columns([col for col in lang_ds[\"train\"].column_names if col not in keep_columns])  \n",
        "    # rename the columns to match transformers schema\n",
        "    lang_ds = lang_ds.rename_column(\"utterance\", \"text\")\n",
        "    lang_ds = lang_ds.rename_column(\"intent\", \"label\")\n",
        "    proc_lan_dataset_list.append(lang_ds)\n",
        "    \n",
        "# concat single splits into one\n",
        "train_dataset = concatenate_datasets([ds[\"train\"] for ds in proc_lan_dataset_list])\n",
        "eval_dataset = concatenate_datasets([ds[\"validation\"] for ds in proc_lan_dataset_list])\n",
        "test_dataset = concatenate_datasets([ds[\"test\"] for ds in proc_lan_dataset_list])\n",
        "# create datset dict for easier processing\n",
        "dataset = DatasetDict(dict(train=train_dataset,validation=eval_dataset,test=test_dataset))\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "PQcreAZumk6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "\n",
        "df.hist()"
      ],
      "metadata": {
        "id": "yFr30fvymmvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "RlHzQhr7mocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "       examples[\"text\"], padding=\"max_length\", truncation=True\n",
        "    )\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(process, batched=True)\n",
        "tokenized_datasets[\"train\"].features"
      ],
      "metadata": {
        "id": "T_McJczXmpuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "# define metrics and metrics function\n",
        "f1_metric = load_metric(\"f1\")\n",
        "accuracy_metric = load_metric( \"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "H-1YREUxmrKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification,DataCollatorWithPadding\n",
        "#from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "# create label2id, id2label dicts for nice outputs for the model\n",
        "labels = tokenized_datasets[\"train\"].features[\"label\"].names\n",
        "num_labels = len(labels)\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ],
      "metadata": {
        "id": "1LG_-699mskG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "#model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_labels, \n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "jlxAnT3rmt4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    repository_id,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        "    save_strategy=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "c9q-Desbmwyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "uEnzq5Agmz2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(tokenized_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "K05ALQW2m6Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "O0uh0t63m7xH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}