{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTJnMbeYJYiG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers datasets seqeval #tensorboard matplotlib pandas sklearn\n",
        "! apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "zdZLVlv4J-vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"xlm-roberta-base\"\n",
        "\n",
        "dataset_id = \"cartesinus/leyzer-fedcsis\"\n",
        "dataset_configs=[\"en-US\"] #,\"de-DE\",\"fr-FR\",\"it-IT\",\"pt-PT\",\"es-ES\",\"nl-NL\"]\n",
        "\n",
        "repository_id = \"fedcsis-slot_baseline-xlm_r-en\""
      ],
      "metadata": {
        "id": "OrYn_5jkKEse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset preparation"
      ],
      "metadata": {
        "id": "U3F_LVFBKOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "from datasets import Features, Sequence, Value, ClassLabel\n",
        "import numpy as np\n",
        "\n",
        "# the columns we want to keep in the dataset\n",
        "keep_columns = [\"utterance\", \"bio\"]\n",
        "\n",
        "def convert_to_bio(sentence):\n",
        "    bio = \"\"\n",
        "    in_slot = False\n",
        "    slot = \"\"\n",
        "    raw_sentence = \"\"\n",
        "    for word in sentence.split(' '):\n",
        "        word = word.strip()\n",
        "        if word.startswith('['):\n",
        "            in_slot = True\n",
        "            b_slot = True\n",
        "            slot = word[1:].lower()\n",
        "        elif word == ':' and in_slot:\n",
        "            continue\n",
        "        elif word.endswith(']'):\n",
        "            in_slot = False\n",
        "            if b_slot:\n",
        "                b_slot = False\n",
        "                bio += \"B-\" + slot.lower() + \" \"\n",
        "            else:\n",
        "                bio += \"I-\" + slot.lower() + \" \"\n",
        "            raw_sentence += word[:-1] + \" \"\n",
        "        elif in_slot:\n",
        "            if b_slot:\n",
        "                b_slot = False\n",
        "                bio += \"B-\" + slot.lower() + \" \"\n",
        "            else:\n",
        "                bio += \"I-\" + slot.lower() + \" \"\n",
        "            raw_sentence += word + \" \"\n",
        "        else:\n",
        "            bio += \"O \"\n",
        "            raw_sentence += word + \" \"\n",
        "\n",
        "    return bio.strip()\n",
        "\n",
        "\n",
        "def convert_to_flattag(sentence):\n",
        "    flattag = \"\"\n",
        "    in_slot = False\n",
        "    slot = \"\"\n",
        "    raw_sentence = \"\"\n",
        "    for word in sentence.split(' '):\n",
        "        word = word.strip()\n",
        "        if word.startswith('['):\n",
        "            in_slot = True\n",
        "            b_slot = True\n",
        "            slot = word[1:].lower()\n",
        "        elif word == ':' and in_slot:\n",
        "            continue\n",
        "        elif word.endswith(']'):\n",
        "            in_slot = False\n",
        "            if b_slot:\n",
        "                b_slot = False\n",
        "                flattag += slot.lower() + \" \"\n",
        "            else:\n",
        "                flattag += slot.lower() + \" \"\n",
        "            raw_sentence += word[:-1] + \" \"\n",
        "        elif in_slot:\n",
        "            if b_slot:\n",
        "                b_slot = False\n",
        "                flattag += slot.lower() + \" \"\n",
        "            else:\n",
        "                flattag += slot.lower() + \" \"\n",
        "            raw_sentence += word + \" \"\n",
        "        else:\n",
        "            flattag += \"O \"\n",
        "            raw_sentence += word + \" \"\n",
        "\n",
        "    return flattag.strip()\n",
        "\n",
        "\n",
        "def get_all_iob_tokens(dataset):\n",
        "    uniq_iob = []\n",
        "    for x in dataset:\n",
        "        for iob_token in x.split(' '):\n",
        "            if not iob_token in uniq_iob:\n",
        "                uniq_iob.append(iob_token)\n",
        "    return uniq_iob\n",
        "\n",
        "\n",
        "# process individuell datasets\n",
        "proc_lan_dataset_list=[]\n",
        "iob = []\n",
        "\n",
        "for lang in dataset_configs:\n",
        "    # load dataset for language\n",
        "    lang_ds = load_dataset(dataset_id, lang)\n",
        "    # only keep the 'utt' & 'scenario column\n",
        "    lang_ds = lang_ds.remove_columns([col for col in lang_ds[\"train\"].column_names if col not in keep_columns])\n",
        "    # rename the columns to match transformers schema\n",
        "    lang_ds = lang_ds.rename_column(\"utterance\", \"text\")\n",
        "    lang_ds = lang_ds.rename_column(\"bio\", \"bio_raw\")\n",
        "\n",
        "    bio_uniq = get_all_iob_tokens([s for s in lang_ds[\"train\"][\"bio_raw\"] + lang_ds[\"test\"][\"bio_raw\"] + lang_ds[\"validation\"][\"bio_raw\"]])\n",
        "    bio = ClassLabel(num_classes=len(bio_uniq), names=bio_uniq)\n",
        "\n",
        "    #tokens\n",
        "    print(lang_ds[\"train\"][\"text\"])\n",
        "    lang_ds[\"train\"] = lang_ds[\"train\"].add_column(\"tokens\", [x.split() for x in lang_ds[\"train\"][\"text\"]])\n",
        "    lang_ds[\"test\"] = lang_ds[\"test\"].add_column(\"tokens\", [x.split() for x in lang_ds[\"test\"][\"text\"]])\n",
        "    lang_ds[\"validation\"] = lang_ds[\"validation\"].add_column(\"tokens\", [x.split() for x in lang_ds[\"validation\"][\"text\"]])\n",
        "    #lang_ds[\"validation\"] = lang_ds[\"validation\"].add_column(\"tokens\", [x.split() for x in lang_ds[\"validation\"][\"text\"]])\n",
        "\n",
        "    #iob\n",
        "    lang_ds[\"train\"] = lang_ds[\"train\"].add_column(\"bio_tokens\", [s.split() for s in lang_ds[\"train\"][\"bio_raw\"]])\n",
        "    lang_ds[\"train\"] = lang_ds[\"train\"].add_column(\"bio\", [bio.str2int(s) for s in lang_ds[\"train\"][\"bio_tokens\"]])\n",
        "\n",
        "    lang_ds[\"test\"] = lang_ds[\"test\"].add_column(\"bio_tokens\", [s.split() for s in lang_ds[\"test\"][\"bio_raw\"]])\n",
        "    lang_ds[\"test\"] = lang_ds[\"test\"].add_column(\"bio\", [bio.str2int(s) for s in lang_ds[\"test\"][\"bio_tokens\"]])\n",
        "\n",
        "    lang_ds[\"validation\"] = lang_ds[\"validation\"].add_column(\"bio_tokens\", [s.split() for s in lang_ds[\"validation\"][\"bio_raw\"]])\n",
        "    lang_ds[\"validation\"] = lang_ds[\"validation\"].add_column(\"bio\", [bio.str2int(s) for s in lang_ds[\"validation\"][\"bio_tokens\"]])\n",
        "\n",
        "    proc_lan_dataset_list.append(lang_ds)\n",
        "\n",
        "\n",
        "# concat single splits into one\n",
        "train_dataset = concatenate_datasets([ds[\"train\"] for ds in proc_lan_dataset_list])\n",
        "test_dataset = concatenate_datasets([ds[\"test\"] for ds in proc_lan_dataset_list])\n",
        "eval_dataset = concatenate_datasets([ds[\"validation\"] for ds in proc_lan_dataset_list]) \n",
        "\n",
        "# create datset dict for easier processing\n",
        "dataset = DatasetDict(dict(train=train_dataset,validation=eval_dataset,test=test_dataset))\n",
        "print(dataset['train'].features) #['iob'][0]\n",
        "print(dataset['train'][200])"
      ],
      "metadata": {
        "id": "oIVfRXcEKSlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenization"
      ],
      "metadata": {
        "id": "-_uug93NKj19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "tokenized_input = tokenizer(dataset[\"train\"][\"text\"], is_split_into_words=True, padding=True, max_length=512, truncation=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "URtY1xLtKjGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### token alignment with labels"
      ],
      "metadata": {
        "id": "Jdie0gJEKpyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_all_tokens = True\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"bio\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx and word_idx < len(label):\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx < len(label):\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            else:\n",
        "            ##this is nasty hack to skip some problematical (probably due to errors in dataset) cases\n",
        "                print(\"label\", label)\n",
        "                print(\"word_idx\", word_idx)\n",
        "                continue\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "HC-vMcz2Kp7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model details and train"
      ],
      "metadata": {
        "id": "uOmmcthqK29S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification,DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "#from huggingface_hub import HfFolder\n",
        "\n",
        "# create label2id, id2label dicts for nice outputs for the model\n",
        "num_labels = bio.num_classes\n",
        "labels = bio.names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_id,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "T2JURB_SK6Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "label_list = labels\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "2yL4r4WyLGf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### training args"
      ],
      "metadata": {
        "id": "DxRCE8lNLIFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    repository_id,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        "#    hub_strategy = \"all_checkpoints\", #for future testing\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yT9mu-yLLJzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### evaluate model"
      ],
      "metadata": {
        "id": "K4q5LZRpLaex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_dataset(dataset):\n",
        "    predictions, eval_labels, _ = trainer.predict(dataset)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, eval_labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, eval_labels)\n",
        "    ]\n",
        "\n",
        "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    results\n",
        "\n",
        "evaluate_on_dataset(tokenized_datasets[\"validation\"])"
      ],
      "metadata": {
        "id": "d5yFZWa-LdKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_on_dataset(tokenized_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "GTIenCxWLi2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### send model to huggingface"
      ],
      "metadata": {
        "id": "KZOtoJBiLWGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "_NH0iNkZLY0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}